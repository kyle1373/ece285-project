{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5630c70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from preprocess import extract_images\n",
    "from dataset import get_full_list, ChineseCharacterDataset\n",
    "from models import CVAE\n",
    "from utils import visualize_images, show_images, plot_generated_images, pad_to_target_size\n",
    "from loss import vae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8f76688",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b82c0d",
   "metadata": {},
   "source": [
    "### Loading and Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "383085cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory and load the dataset\n",
    "image_dir = './chinese_chars/pngs'\n",
    "full_data_list = get_full_list(image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e134457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Size: 9574\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(full_data_list))\n",
    "print(\"Training Set Size:\", train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93b1ee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ChineseCharacterDataset(full_data_list[:train_size], cond_type='Row', rows=[20,43])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a40d89",
   "metadata": {},
   "source": [
    "### Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a8bb9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2fbecde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "def train(model, train_data_set, input_dim, optimizer, scheduler, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, condition) in enumerate(train_data_set):\n",
    "            data = data.to(device)\n",
    "            condition = condition.to(device)\n",
    "            padded_condition = pad_to_target_size(condition, (64, 64)).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed_batch, mu, logvar = model(data, padded_condition)\n",
    "            loss = vae_loss(reconstructed_batch.view(data.shape[0],1,64,64), data, mu, logvar)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(f'Epoch {epoch+1}, Loss: {train_loss/len(train_data_set.dataset)}')\n",
    "    print(f'Final Loss: {train_loss/len(train_data_set.dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c18358f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1734.7816492339277\n",
      "Epoch 2, Loss: 1408.894734182013\n",
      "Epoch 3, Loss: 1394.9308922908888\n",
      "Epoch 4, Loss: 1378.8421255238798\n",
      "Epoch 5, Loss: 1367.3558959833435\n",
      "Epoch 6, Loss: 1351.2693015425893\n",
      "Epoch 7, Loss: 1339.6957619696736\n",
      "Epoch 8, Loss: 1328.135917426102\n",
      "Epoch 9, Loss: 1317.3303180835628\n",
      "Epoch 10, Loss: 1307.6427123079923\n",
      "Epoch 11, Loss: 1291.252030646151\n",
      "Epoch 12, Loss: 1271.71491581407\n",
      "Epoch 13, Loss: 1255.7590383542015\n",
      "Epoch 14, Loss: 1235.8538233125685\n",
      "Epoch 15, Loss: 1211.0373370423804\n",
      "Epoch 16, Loss: 1181.8956076719828\n",
      "Epoch 17, Loss: 1154.9805920188303\n",
      "Epoch 18, Loss: 1130.7025966831557\n",
      "Epoch 19, Loss: 1107.3552968264473\n",
      "Epoch 20, Loss: 1084.8433331198103\n",
      "Epoch 21, Loss: 1062.1705125147698\n",
      "Epoch 22, Loss: 1042.746285767851\n",
      "Epoch 23, Loss: 1026.9685948046956\n",
      "Epoch 24, Loss: 1013.7911331817455\n"
     ]
    }
   ],
   "source": [
    "input_dim = 64 * 64\n",
    "z_dim = 50\n",
    "condition_dim = 50\n",
    "hidden_dim = 6400*8\n",
    "learning_rate = 1e-3\n",
    "\n",
    "num_epochs = 25\n",
    "\n",
    "model = CVAE(input_dim, hidden_dim, z_dim, condition_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "train(model, train_loader, input_dim, optimizer, scheduler, num_epochs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d384e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './saves/cvae_r20_43_extra.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4e330e",
   "metadata": {},
   "source": [
    "### Generating Images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d6035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_generated_images(model, data_loader, num_images, batch_size, device='cpu'):\n",
    "    model.eval()\n",
    "    left_images = num_images\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, condition) in enumerate(data_loader):\n",
    "            if batch_idx < 18:\n",
    "                continue\n",
    "            condition = condition.to(device)\n",
    "            data = data.to(device)\n",
    "            if left_images == 0:\n",
    "                break\n",
    "                \n",
    "            \n",
    "            z = torch.randn(batch_size, model.z_dim).to(device)\n",
    "            padded_condition = pad_to_target_size(condition, (64, 64)).to(device)\n",
    "            cond_encoded = model.forward_condition_encoder(padded_condition)\n",
    "            sample = model.decode(z, cond_encoded).cpu()\n",
    "            sample = sample.view(batch_size, 1, 64, 64)\n",
    "            \n",
    "            if left_images > batch_size:\n",
    "                print_images = batch_size\n",
    "                left_images = left_images-batch_size\n",
    "            else:\n",
    "                print_images = left_images\n",
    "                left_images = 0\n",
    "\n",
    "            for i in range(print_images):\n",
    "                ref = data[i].cpu().detach().numpy().reshape(64, 64)\n",
    "                img = sample[i].cpu().detach().numpy().reshape(64, 64)\n",
    "                \n",
    "                # Handle condition image with random shape\n",
    "                cond = condition[i].cpu().detach().numpy()\n",
    "                if len(cond.shape) > 2:\n",
    "                    cond = cond[0]  # Select the first channel if condition is multi-channel\n",
    "                cond_shape = cond.shape\n",
    "                cond_resized = cond.reshape(cond_shape)\n",
    "\n",
    "                plt.figure(figsize=(12, 4))\n",
    "\n",
    "                # Plot condition image\n",
    "                plt.subplot(1, 3, 1)\n",
    "                plt.title('Condition Image')\n",
    "                plt.imshow(cond_resized, cmap='gray')\n",
    "                plt.axis('off')\n",
    "\n",
    "                # Plot reference image\n",
    "                plt.subplot(1, 3, 2)\n",
    "                plt.title('Reference Image')\n",
    "                plt.imshow(ref, cmap='gray')\n",
    "                plt.axis('off')\n",
    "\n",
    "                # Plot generated image\n",
    "                plt.subplot(1, 3, 3)\n",
    "                plt.title('Generated Image')\n",
    "                plt.imshow(img, cmap='gray')\n",
    "                plt.axis('off')\n",
    "\n",
    "                plt.show()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10292d84",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_generated_images(model, train_loader, num_images=5\n",
    "                      , batch_size=32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6ee7ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b2d900",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
