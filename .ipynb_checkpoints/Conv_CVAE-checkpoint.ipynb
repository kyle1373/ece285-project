{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5630c70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from preprocess import extract_images\n",
    "from dataset import get_full_list, ChineseCharacterDataset\n",
    "from models import CVAE\n",
    "from utils import visualize_images, show_images, plot_generated_images, pad_to_target_size\n",
    "from loss import vae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8f76688",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b82c0d",
   "metadata": {},
   "source": [
    "### Loading and Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93b1ee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory and load the dataset\n",
    "image_dir = './chinese_chars/pngs'\n",
    "full_data_list = get_full_list(image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "800fd534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Size: 9574\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(full_data_list))\n",
    "print(\"Training Set Size:\", train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7db53a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ChineseCharacterDataset(full_data_list[:train_size], cond_type='Row', rows=[20,43])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450d5651",
   "metadata": {},
   "source": [
    "### Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae239b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Conv_CVAE(nn.Module):\n",
    "    def __init__(self, z_dim, condition_dim):\n",
    "        super(Conv_CVAE, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.condition_dim = condition_dim\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(2, 32, 4, stride=2, padding=1),  # Change input channels to 2 for concatenated input\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, z_dim)\n",
    "        self.fc2 = nn.Linear(128 * 8 * 8, z_dim)\n",
    "\n",
    "        # Condition encoder\n",
    "        self.condition_encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 8 * 8, condition_dim) \n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(z_dim + condition_dim, 128 * 8 * 8)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(1, (128, 8, 8)),\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, 4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x, condition):\n",
    "        # Concatenate the condition with the input image\n",
    "        x_cond = torch.cat([x, condition], dim=1)\n",
    "        h1 = self.encoder(x_cond)\n",
    "        return self.fc1(h1), self.fc2(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, condition):\n",
    "        # Encode the condition\n",
    "        cond_encoded = self.condition_encoder(condition)\n",
    "        # Concatenate z with the encoded condition\n",
    "        z_cond = torch.cat([z, cond_encoded], dim=1)\n",
    "        h3 = F.relu(self.fc3(z_cond))\n",
    "        return self.decoder(h3)\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        mu, logvar = self.encode(x, condition)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z, condition), mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a40d89",
   "metadata": {},
   "source": [
    "### Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a8bb9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2fbecde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "def train(model, train_data_set, input_dim, optimizer, scheduler, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, condition) in enumerate(train_data_set):\n",
    "            data = data.to(device)\n",
    "            condition = condition.to(device)\n",
    "            padded_condition = pad_to_target_size(condition, (64, 64)).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed_batch, mu, logvar = model(data, padded_condition)\n",
    "            loss = vae_loss(reconstructed_batch, data, mu, logvar)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(f'Epoch {epoch+1}, Loss: {train_loss/len(train_data_set.dataset)}')\n",
    "    print(f'Final Loss: {train_loss/len(train_data_set.dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c18358f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_dim = 64 * 64\n",
    "z_dim = 50\n",
    "condition_dim = 50\n",
    "learning_rate = 1e-4\n",
    "\n",
    "num_epochs = 25\n",
    "\n",
    "model = Conv_CVAE(z_dim, condition_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "train(model, train_loader, input_dim, optimizer, scheduler, num_epochs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42a0d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './saves/unet_cvae_r20_43.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4e330e",
   "metadata": {},
   "source": [
    "### Generating Images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10292d84",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def plot_generated_images(model, data_loader, num_images, batch_size, device='cpu'):\n",
    "    model.eval()\n",
    "    left_images = num_images\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, condition) in enumerate(data_loader):\n",
    "            condition = condition.to(device)\n",
    "            data = data.to(device)\n",
    "            if left_images == 0:\n",
    "                break\n",
    "            \n",
    "            z = torch.randn(batch_size, model.z_dim).to(device)\n",
    "            sample = model.decode(z, condition).cpu()\n",
    "            sample = sample.view(batch_size, 1, 64, 64)\n",
    "            \n",
    "            if left_images > batch_size:\n",
    "                print_images = batch_size\n",
    "                left_images = left_images-batch_size\n",
    "            else:\n",
    "                print_images = left_images\n",
    "                left_images = 0\n",
    "                \n",
    "\n",
    "            for i in range(print_images):\n",
    "                ref = data[i].cpu().detach().numpy().reshape(64, 64)\n",
    "                img = sample[i].cpu().detach().numpy().reshape(64, 64)\n",
    "\n",
    "                plt.figure(figsize=(8, 4))\n",
    "\n",
    "                # Plot reference image\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.title('Reference Image')\n",
    "                plt.imshow(ref, cmap='gray')\n",
    "                plt.axis('off')\n",
    "\n",
    "                # Plot generated image\n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.title('Generated Image')\n",
    "                plt.imshow(img, cmap='gray')\n",
    "                plt.axis('off')\n",
    "\n",
    "                plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `model` is an instance of CVAE and `train_loader` is your DataLoader\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "plot_generated_images(model, train_loader, num_images=5, batch_size=32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6ee7ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
